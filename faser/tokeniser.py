from typing import List, Tuple

from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.trainers import BpeTrainer


def train_bpe_tokeniser(
    training_files: List[str],
    specials: Tuple[None, List[str]] = None,
    max_seq_length: int = 256,
) -> Tokenizer:
    """
    Train a byte pair encoding (BPE) tokeniser

    Args:
        training_files: A list containing paths to training files to train the tokeniser
        specials: A list of special tokens to add to the tokeniser (defaults are
                  "[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]" if left as None)

    Returns:
        A trained BPE tokeniser object
    """
    if specials is None:
        specials = ["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]

    tokenizer = Tokenizer(BPE())
    tokenizer.pre_tokenizer = Whitespace()
    tokenizer.enable_padding(length=max_seq_length)
    tokenizer.enable_truncation(max_length=max_seq_length)
    trainer = BpeTrainer(special_tokens=specials)
    if isinstance(training_files, list):
        tokenizer.train(files=training_files, trainer=trainer)
    else:
        raise TypeError(f"Training files is expected to be a List of string")

    return tokenizer


def print_tokeniser_encode_example(tokeniser: Tokenizer, str_to_encode: str) -> None:
    """
    Print the tokens and ids generated by a given tokeniser from a given input string
    Args:
        tokeniser: A Tokeniser object representing a trained HuggingFace Tokeniser
        str_to_encode: A string which will be tokenised

    Returns:
        None
    """
    output = tokeniser.encode(str_to_encode)
    print(output)
    print(f"Tokens: {output.tokens}  ({len(output.tokens)})")
    print(f"Ids: {output.ids} ({len(output.ids)})")


if __name__ == "__main__":
    tokeniser = train_bpe_tokeniser(["data-samples/tokenisers/efs_for_tokeniser.txt"])
    print_tokeniser_encode_example(
        tokeniser, "15 r3 == $z zf := 31 $s nf := 32 $b ! cf"
    )

    tokeniser = train_bpe_tokeniser(["data-samples/tokenisers/dfs_for_tokenizer.txt"])
    print_tokeniser_encode_example(
        tokeniser, "cmp reg64 0xf ja MEM mov reg64 reg64 push"
    )
